{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pathlib as pl\n",
    "import datetime as dt\n",
    "from functools import reduce #needed to allow merging of multiple datasets      \n",
    "\n",
    "home = pl.Path(r'C:\\Users\\kevin.omalley\\OneDrive - University of Limerick\\Documents\\GitHub\\fino2py\\data')\n",
    "p1 = pl.Path(r'C:\\Users\\kevin.omalley\\OneDrive - University of Limerick\\Documents\\GitHub\\fino2py\\data\\Participant 1_2022-10-27_09.07.37')\n",
    "p2 = pl.Path(r'C:\\Users\\kevin.omalley\\OneDrive - University of Limerick\\Documents\\GitHub\\fino2py\\data\\Partcipant 59_2022-11-21_10.37.29')\n",
    "time_stamps = pl.Path(r'C:\\Users\\kevin.omalley\\OneDrive - University of Limerick\\Documents\\GitHub\\fino2py\\data\\Timesheets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_finometer_intervals_feb_23(folder_path, interval='1T', save_csv=False):\n",
    "    '''This function imports the finometer data and then calculates the average of each measure over the selected time period'''\n",
    "\n",
    "    if not isinstance(folder_path, pl.Path): # check if folder_path is a pathlib.Path object\n",
    "        raise TypeError('folder_path must be a pathlib.Path object')\n",
    "    elif not folder_path.exists() or not folder_path.is_dir(): # check if folder_path exists and is a directory\n",
    "        raise ValueError('folder_path does not exist or is not a directory')\n",
    "    else:\n",
    "        # find the .txt file in the folder\n",
    "        files = [file for file in folder_path.glob('*.txt')]\n",
    "        if len(files) != 1:\n",
    "            raise ValueError(f'Expected one .txt file, but found {len(files)} in the folder')\n",
    "        file = files[0]\n",
    "\n",
    "        # read in the data from the .txt file\n",
    "        df = pd.read_csv(\n",
    "            file,\n",
    "            sep=';',\n",
    "            header=0,\n",
    "            skiprows=8,\n",
    "            skipfooter=1,\n",
    "            engine='python',\n",
    "        )\n",
    "\n",
    "        # drop the unnamed column at position 13\n",
    "        df = df.drop(df.columns[13], axis=1)\n",
    "\n",
    "        # set the 'Time (s)' column to a datetime object\n",
    "        df['Time (s)'] = pd.to_datetime(df['Time (s)'], format='%H:%M:%S.%f')\n",
    "\n",
    "        # set the 'Time (s)' column as the index and resample the DataFrame\n",
    "        df = df.set_index('Time (s)').resample(interval).mean()\n",
    "\n",
    "        # format the DatetimeIndex to only show hours and minutes\n",
    "        # df.index = df.index.strftime('%H:%M')\n",
    "\n",
    "        if save_csv:\n",
    "            df.to_csv(folder_path / f'imported data for {file.stem.split(\"_\")[0]}.csv', index=True)\n",
    "            print(f'CSV saved for {file.stem.split(\"_\")[0]}')\n",
    "\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[import_finometer_intervals_feb_23(i, save_csv=True) for i in home.iterdir() if i.is_dir()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_protocol_times(times_file_path, add_seconds=False, flatten_seconds=False, save_csv=False):\n",
    "    '''This function imports the protocol times from the .csv file and returns a dataframe with the protocol times for each participant\n",
    "    \n",
    "    file_path: pathlib.Path object\n",
    "        The path to the .csv file containing the protocol times\n",
    "    add_seconds: boolean (optional)\n",
    "        If True, seconds will be added to the time values (if missing)\n",
    "    save_csv: boolean (optional)\n",
    "        If True, the imported data will be saved as a .csv file in the same folder as the .csv file, this is not always needed and should be used sparingly\n",
    "    flatten_seconds: boolean (optional)\n",
    "        If True, seconds will be set to 00 for all time values\n",
    "    '''\n",
    "\n",
    "    def add_seconds_to_time(time_str):\n",
    "        '''This function adds seconds to the time string for in case the time string is missing seconds'''\n",
    "        if len(time_str) == 5:\n",
    "            time_str += \":00\"\n",
    "        return time_str\n",
    "\n",
    "    def flatten_seconds(time_str):\n",
    "        '''This function sets seconds to 00 for a given time string'''\n",
    "        return time_str[:5] + ':00'\n",
    "\n",
    "    if not isinstance(times_file_path, pl.Path):#check if folder_path is a pathlib.Path object\n",
    "        raise TypeError('file_path must be a pathlib.Path object')\n",
    "    elif not times_file_path.exists(): #  and if it exists\n",
    "        raise ValueError('file_path does not exist')\n",
    "    elif not times_file_path.is_file(): #  and is a file \n",
    "        raise ValueError('file_path is not a file')\n",
    "    elif times_file_path.suffix != '.csv': #  and is a csv file\n",
    "        raise ValueError('file_path is not an csv file')\n",
    "    else:\n",
    "        df = pd.read_csv(times_file_path, delimiter= ',')\n",
    "        df.columns = [col.strip() for col in df.columns]\n",
    "        cols_to_keep = ['Participant ID', 'Start of Baseline', 'End of Baseline', 'Start of Task 1', 'End of Task 1', 'Start of Recovery Period', 'End of Recovery Period']\n",
    "        df = df[cols_to_keep].applymap(lambda x: str(x).strip('\"') if isinstance(x, str) else x)\n",
    "        \n",
    "        if add_seconds and flatten_seconds:\n",
    "            raise ValueError('Only one of add_seconds and flatten_seconds can be True')\n",
    "\n",
    "        if add_seconds:\n",
    "            try:\n",
    "                df.iloc[:, 1:] = df.iloc[:, 1:].applymap(lambda x: add_seconds_to_time(x) if isinstance(x, str) else x)\n",
    "                df.iloc[:, 1:] = df.iloc[:, 1:].applymap(lambda x: pd.to_datetime(x, format='%H:%M:%S', errors='coerce'))\n",
    "            except:\n",
    "                print('Could not add seconds to time, please check the time format')\n",
    "\n",
    "        elif flatten_seconds:\n",
    "            try:\n",
    "                df.iloc[:, 1:] = df.iloc[:, 1:].applymap(lambda x: add_seconds_to_time(x) if isinstance(x, str) else x)\n",
    "                df.iloc[:, 1:] = df.iloc[:, 1:].applymap(lambda x: flatten_seconds(x) if isinstance(x, str) else x)\n",
    "                df.iloc[:, 1:] = df.iloc[:, 1:].applymap(lambda x: pd.to_datetime(x, format='%H:%M:%S', errors='coerce'))\n",
    "            except:\n",
    "                print('Could not set seconds to 00, please check the time format')\n",
    "        \n",
    "        if save_csv: #if you want to save the csv file (which may be useful if you want to use the data in other ways)\n",
    "            try:\n",
    "                df.to_csv(times_file_path.parent / f\"cleaned times.csv\", index=False)\n",
    "                print(f\"CSV saved for {times_file_path.stem}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Could not save csv file, error: {e}\")\n",
    "            \n",
    "\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = import_protocol_times(\n",
    "    time_stamps,\n",
    "    flatten_seconds = True,\n",
    "    save_csv=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_protocol_averages_feb_23(file_path, t1=None, t2=None, t3=None, t4=None, t5=None, t6=None, save_csv=False):\n",
    "    '''A function that imports the averaged finomter files (which have already been processed from the raw data)\n",
    "    to produce averages for each section of the experiment protocol'''\n",
    "\n",
    "    if not isinstance(file_path, pl.Path):\n",
    "        raise TypeError('file_path must be a pathlib.Path object')\n",
    "    elif not file_path.exists():\n",
    "        raise ValueError('file_path does not exist')\n",
    "    else:\n",
    "        # check if the file is a csv or xlsx file\n",
    "        if file_path.suffix == '.csv':\n",
    "            try:\n",
    "                intervals = pd.read_csv(file_path, index_col=0, parse_dates=True)\n",
    "            except pd.errors.EmptyDataError:\n",
    "                print(f\"File {file_path} is empty\")\n",
    "        elif file_path.suffix == '.xlsx':\n",
    "            try:\n",
    "                intervals = pd.read_excel(file_path, index_col=0, parse_dates=True)\n",
    "            except pd.errors.EmptyDataError:\n",
    "                print(f\"File {file_path} is empty\")\n",
    "        else:\n",
    "            raise ValueError(\"File must be a Path object for a .csv or .xlsx file\")\n",
    "\n",
    "    intervals.index = pd.to_datetime(intervals.index).strftime('%H:%M:%S')\n",
    "    intervals = intervals.replace('--', np.nan)\n",
    "    intervals = intervals.dropna(how='all')\n",
    "\n",
    "    def create_protocol_part_df(frame, start, end, tag, filename):\n",
    "        '''A function that creates a dataframe representing a time period within a particular experimental protocol from a given DataFrame.\n",
    "        Takes a DataFrame, start and end times for the portion of the experimental protocol.\n",
    "        Returns a DataFrame with the mean values of the given columns during that portion of the study.'''\n",
    "        period = frame.loc[start:end].dropna().T\n",
    "        period['Average'] = period.mean(axis=1)\n",
    "        b = pd.DataFrame(period['Average']).T\n",
    "        b.columns = [f'{i}_{tag}' for i in b.columns]\n",
    "        b['Participant ID'] = f'{filename}'\n",
    "        return b\n",
    "\n",
    "    # creating the baseline dataframe\n",
    "    baseline = create_protocol_part_df(intervals, t1 , t2, 'bl', file_path.parent.stem.split('_')[0])\n",
    "\n",
    "    # creating the task dataframe\n",
    "    task = create_protocol_part_df(intervals, t3, t4, 'task', file_path.parent.stem.split('_')[0])\n",
    "\n",
    "    # creating the recovery dataframe\n",
    "    recovery = create_protocol_part_df(intervals, t5, t6, 'rec', file_path.parent.stem.split('_')[0])\n",
    "\n",
    "    # creating a dataframe from the smaller dataframes\n",
    "    data_merge = reduce(lambda left, right: pd.merge(left , right, on = [\"Participant ID\"], how = \"outer\"), [baseline, task, recovery])\n",
    "    data_merge.set_index('Participant ID', inplace=True)\n",
    "\n",
    "    if save_csv:\n",
    "        data_merge.to_csv(file_path.parent / f\"{file_path.parent.stem.split('_')[0]} protocol_averages.csv\")\n",
    "        print(f\"Saved {file_path.parent.stem.split('_')[0]} protocol averages.csv to {file_path.parent}\")\n",
    "    \n",
    "    return data_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for n in y.index:\n",
    "    if y.iloc[n, 1:].isnull().values.any():\n",
    "        print(f\"Skipping participant {y.loc[n, 'Participant ID']} due to missing timestamps\")\n",
    "        continue\n",
    "    participant_id = y.loc[n, 'Participant ID']\n",
    "    file_name = f'imported data for {participant_id}.csv'\n",
    "    file_paths = list(home.glob(f'**/{file_name}'))\n",
    "    if not file_paths:\n",
    "        print(f'no file found for {participant_id}')\n",
    "    else:\n",
    "        full_path = file_paths[0].resolve()\n",
    "        print(f'File {full_path} found')\n",
    "\n",
    "        start_bl = y.loc[n, 'Start of Baseline']\n",
    "        end_bl = y.loc[n, 'End of Baseline']\n",
    "\n",
    "        start_task = y.loc[n, 'Start of Task 1']\n",
    "        end_task = y.loc[n, 'End of Task 1']\n",
    "\n",
    "        start_rec = y.loc[n, 'Start of Recovery Period']\n",
    "        end_rec = y.loc[n, 'End of Recovery Period']\n",
    "\n",
    "        import_path = f'imported data for {participant_id}.csv'\n",
    "\n",
    "        import_protocol_averages_feb_23(full_path,\n",
    "                                        t1=start_bl.strftime('%H:%M:%S'),\n",
    "                                        t2=end_bl.strftime('%H:%M:%S'),\n",
    "                                        t3=start_task.strftime('%H:%M:%S') if not pd.isna(start_task) else None,\n",
    "                                        t4=end_task.strftime('%H:%M:%S') if not pd.isna(end_task) else None,\n",
    "                                        t5=start_rec.strftime('%H:%M:%S') if not pd.isna(start_rec) else None,\n",
    "                                        t6=end_rec.strftime('%H:%M:%S') if not pd.isna(end_rec) else None,\n",
    "                                        save_csv=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_protocol_rows(data_path, timestamps_df):\n",
    "    \"\"\"\n",
    "    Import data for participants whose timestamps are all non-null.\n",
    "    \n",
    "    Parameters:\n",
    "    - data_path: pl.Path or str, the path to the folder containing the data files\n",
    "    - timestamps_df: pandas DataFrame, containing the timestamps for each participant\n",
    "    \n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    data = pl.Path(data_path)\n",
    "    for n in timestamps_df.index:\n",
    "        if timestamps_df.iloc[n, 1:].isnull().values.any():\n",
    "            print(f\"Skipping participant {timestamps_df.loc[n, 'Participant ID']} due to missing timestamps\")\n",
    "            continue\n",
    "        participant_id = timestamps_df.loc[n, 'Participant ID']\n",
    "        file_name = f'imported data for {participant_id}.csv'\n",
    "        file_paths = list(data.glob(f'**/{file_name}'))\n",
    "        if not file_paths:\n",
    "            print(f'no file found for {participant_id}')\n",
    "        else:\n",
    "            full_path = file_paths[0].resolve()\n",
    "            print(f'File {full_path} found')\n",
    "\n",
    "            start_bl = timestamps_df.loc[n, 'Start of Baseline']\n",
    "            end_bl = timestamps_df.loc[n, 'End of Baseline']\n",
    "\n",
    "            start_task = timestamps_df.loc[n, 'Start of Task 1']\n",
    "            end_task = timestamps_df.loc[n, 'End of Task 1']\n",
    "\n",
    "            start_rec = timestamps_df.loc[n, 'Start of Recovery Period']\n",
    "            end_rec = timestamps_df.loc[n, 'End of Recovery Period']\n",
    "\n",
    "            import_path = f'imported data for {participant_id}.csv'\n",
    "\n",
    "            import_protocol_averages_feb_23(full_path,\n",
    "                                            t1=start_bl.strftime('%H:%M:%S'),\n",
    "                                            t2=end_bl.strftime('%H:%M:%S'),\n",
    "                                            t3=start_task.strftime('%H:%M:%S') if not pd.isna(start_task) else None,\n",
    "                                            t4=end_task.strftime('%H:%M:%S') if not pd.isna(end_task) else None,\n",
    "                                            t5=start_rec.strftime('%H:%M:%S') if not pd.isna(start_rec) else None,\n",
    "                                            t6=end_rec.strftime('%H:%M:%S') if not pd.isna(end_rec) else None,\n",
    "                                            save_csv=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Zen of Python, by Tim Peters\n",
      "\n",
      "Beautiful is better than ugly.\n",
      "Explicit is better than implicit.\n",
      "Simple is better than complex.\n",
      "Complex is better than complicated.\n",
      "Flat is better than nested.\n",
      "Sparse is better than dense.\n",
      "Readability counts.\n",
      "Special cases aren't special enough to break the rules.\n",
      "Although practicality beats purity.\n",
      "Errors should never pass silently.\n",
      "Unless explicitly silenced.\n",
      "In the face of ambiguity, refuse the temptation to guess.\n",
      "There should be one-- and preferably only one --obvious way to do it.\n",
      "Although that way may not be obvious at first unless you're Dutch.\n",
      "Now is better than never.\n",
      "Although never is often better than *right* now.\n",
      "If the implementation is hard to explain, it's a bad idea.\n",
      "If the implementation is easy to explain, it may be a good idea.\n",
      "Namespaces are one honking great idea -- let's do more of those!\n"
     ]
    }
   ],
   "source": [
    "import this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_protocol_rows(home, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "\n",
    "# Loop over each csv file in the data directory\n",
    "for csv_file in home.glob('**/*protocol_averages.csv'):\n",
    "    # Read in the csv file as a DataFrame and append to the list\n",
    "    df = pd.read_csv(csv_file)\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate the list of DataFrames into a single DataFrame\n",
    "merged = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Print the merged DataFrame\n",
    "merged"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fino2py",
   "language": "python",
   "name": "fino2py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "34fd425172d60228103b60540053fe660871da4661f867ea03bae28a27ac97ff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
